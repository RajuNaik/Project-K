how to check spark version:
v_spark_version = spark.version
print(v_spark_version) --I worked on 3.1.2

default behaviour of spark:
one or more executors allowed per one worked node
one executor will have at max 5 cores

now you can assume AT A TIME, one core per one partition or task

like wise we usually build, 5 partitions per core for better utilization and performance.
======================================================================================================================================================
1.How many executors will you assign for 10GB file in HDFS?
2.How many cores are needed for each executor and what amount of memory required for each executor?

======================================================================================================================================================
1.Spark architecture(how it works internally) --driver node,worker node,DAG,stage,task,cluster manager
		when user submit the application, it will be submitted to driver node as a job
		driver further split the job into multiple stages(stage1, stage2 etc)
		and stage will further divided into tasks, and task will execute the code based on partition. means one task per one partition per one core
		it means, tasks can be executed parallely but stages will have the dependency
		once all tasks completed, then stage will be marked as complete
		once stage1 completes then only stage2 will pick since it has the dependency at data level
======================================================================================================================================================
2.Spark API's(RDD,Dataframe,Dataset) and their differences
	RDD and DataFrame are same wrt, immutability,resilient(fault tolerance ),in-memory, distributed
	differences - RDD does not have schema where dataframe contains schema like relational table in DB
	Since there is no schema concept in RDD, optimization can not be applied, it applies only to DataFrame and datasets
3.what are the transformations? --map,filter,aggregator,sorter,union etc
4.what are the actions? --collect(),write,save,first,foreach,min,max,count etc
5.what is narrow transformation? --no data suffling is required--like filter,union
6.what is wide transformation?--data suffling is required--like group by and joins
======================================================================================================================================================
7.what is on-heap memory?what is off-heap memory? which is faster?
	--on-heap --inside executors,managed by JVM--recommended for small/medium size datasets--faster than off-heap
	--off-heap --outside executors,managed by OS--recommended for large scale datasets --little slow but faster than disk
	--disadvangages --on-heap ->Garbage collectors will take some time to cleanup old/unused objects inside executors when it is full
					--off-heap ->take time for I/O
					
	By default, off-heap is disabled. to enable it, conf spark.memory.offHeap.enabled=true
	you can also, decide the size of your own like - --conf spark.memory.offHeap.size=1g
======================================================================================================================================================8.What is cluster? --computing infrastructure--3 types, all purpose(interactive cluster--for notebooks development),job(for jobs/workflows/production),
					pools--all purpose cluster will take some time for reboot everytime--to avoid this, we go for pool cluster
					in pool cluster, some nodes are always up and runnnig as per configuration
					
9.Types of modes in cluster --single node(only 1 driver node and NO worker nodes--less work loads)--cost cutting
							--multi node(many to many--heavy work loads)


======================================================================================================================================================
10.Read csv file in spark
	inferSchema is not recommended --Reason below:
	Extra Pass Over Data: When reading a CSV file with inferSchema enabled, Spark performs two passes over the data. In the first pass, it reads the data to determine the schema and data types of each column. In the second pass, it reads the data again to create the DataFrame based on the inferred schema. This additional pass can result in increased data reading time.

Hence, always go with schema creation using Struct Type and attach to df while creating

check Spark Reader - Read CSV file notebook
======================================================================================================================================================
11.Handling error records - in 3 ways- while reading from file
		Permissive --allow bad records in separate column
		DROPMALFORMED - will drop bad records
		FAILFAST - throw exception immediately
		
		.option("mode","any of the above")
		--Check notebook for more details
======================================================================================================================================================
12. To connect any database can it be On prem or SQL DB, we need to create JDBC connector(syntax)
eg:
// Define JDBC connection properties
val jdbcUrl = "jdbc:sqlserver://your-server:1433;databaseName=your-database"
val jdbcProperties = new java.util.Properties()
jdbcProperties.setProperty("user", "your-username")
jdbcProperties.setProperty("password", "your-password")

// Define the SQL query
val query = "SELECT * FROM your_table"

// Load data from SQL Server into a DataFrame
val jdbcDF: DataFrame = spark.read.jdbc(jdbcUrl, query, jdbcProperties)
======================================================================================================================================================
Connect Snowflake from databricks and load data:
options = {
  "sfUrl":"https://pr12345.east-us-2.azure.snowflakecomputing.com",
  "sfUser":"snowflake_user_name",
  "sfPassword": "xxxxxx",
  "sfDatabase": "something_db",
  "sfSchema": "sch_something",
  "sfWarehouse": "wh_something"
}

# Create SparkSession
spark = SparkSession.builder \
    .appName("Snowflake Example") \
    .config("spark.jars", "dbfs:/FileStore/jars/b7b979bb_cf60_4739_afc0_418e3d30fbe9-snowflake_jdbc_3_9_2_javadoc-30e86.jar") \ --jar file should be downloaded and placed in this location
    .getOrCreate()

# Read data from DataFrame and write to Snowflake
df.write \
    .format("snowflake") \ --IMP
    .options(**snowflake_options) \
    .option("dbtable", "sch.tab_name") \
    .mode("overwrite") \
    .save()
======================================================================================================================================================
13. To connect any external storages(ADLS, S3 etc) need to create a mount point using dbutils
	eg:
	ACCESS_KEY=df.select('Access key ID').take(1)[0]['Access key ID']    --assuming we stored secret key & access key in a file and accesing using df
	SECRET_KEY=df.select('Secret access key').take(1)[0]['Secret access key']
	
	import urllib
	ENCODED_SECRET_KEY=urllib.parse.quote(string=SECRET_KEY,safe="")
	
	AWS_S3_BUCKET_NAME='naikss3bucket-2'
	MOUNT_NAME='/mnt/mount_aws_s3/'
	SOURCE_URL="s3a://%s:%s@%s" %(ACCESS_KEY,ENCODED_SECRET_KEY,AWS_S3_BUCKET_NAME)
	
	dbutils.fs.mount(SOURCE_URL,MOUNT_NAME)
======================================================================================================================================================	
14. Instead hard coding any sensitive information, we can leverage Azure keyvault concept. First need to create Azure keyvault service in Azure and create secrets in it and access it from Databricks notebook

		accessing keyvault from databricks:
		first we need to create a secret scope using worskapce link and add secrets/createScope at the end.
		enter secret name,DNS(url of Azure KV), ResourceID
		Once secret is created, we can retreive it using dbutils.secret.get(scope=<scope name>, key=<secret name in KV>)
		  eg: password=dbutils.secret.get(scope=<scope name>, key=<secret name in KV>)
		  
################################### Performance Tuning Concepts ######################################################################################
		  
15.By default, when you read a file or from database(INSTEAD OF READING WE CAN CALL IT WRITING INTO MEMORY), 
Spark will partition the data with 128 MB of each partition.
		spark.conf.get("spark.sql.files.maxPartitionBytes") --134217728b i.e 128MB
		
		by default, spark will create partitions when you write into memory(generally creating the df out of a file)		
		so default number of partitions is bases on file size. that is mutliple for 128MB.
		df.rdd.getNumPartitions()
		
		**to see the data inside each paritions --df.rdd.glom().collect()
		But, default partition size can be changed. eg: spark.conf.set("spark.sql.files.maxPartitionBytes",2000) --changed to 2kb per partition
		
		sc.defaultParallelism --to check default no of parallelism --sc means spark context		
		**defaultParallelism == no of cores in a worker node
		
		number of partitions can be changed always using repartition or COALSCE
		
Decide partition size (block size default is 128MB). Based on that it will create no of files at table.

table size				target file size			approx no of files in table
10GB					256MB						40
1TB						256MB						4096
2.56TB					256MB						10240
3TB						307MB						12108
5TB						512MB						17339
7TB						716MB						20784
10TB					1GB							24437

======================================================================================================================================================
16.repartition --means, it will create the partitions evenly. It requires data shuffling b/w partitions
		df.repartition(10,country_id) --create 10 partitions based on countryid
		
		num=df.distinct(countryid).count
		df.repartition(num,"countryid)
		**here number of partitions is mandatory hence calculating it. it all depends on req.
		
	COALSCE - will not shuffle the data but only REDUCE the number of paritions
	
	reparition can increase or decrease the no of paritions where as COALSCE will ONLY decrease the no of partitions
eg:
when we reparitioned df with some 10 paritions, and we want to reduce them TO 5--
coalesced_df = df.coalesce(5)

======================================================================================================================================================
36. IMP - count number of records in each partition.
	for eg, I am reading a file and creating a df(means writing into memory). now spark will create some default partitions based on its file size(each one 128MB)
	
	lets say we have file with 10MB - in that case, it will create only 1 partition by default.
	Now lets change the number of partitions using df.repartition(5) and check how many records are segregated in each partition.
	
	df.repartition(5)
	
	from pyspark.sql.functions import spark_partition_id*****
	df2=df1.withColumn("partitionId",spark_partition_id())  in our case spark_partition_id would be 0,1,2,3,4 (5 partitions)
	
	df2=df1.withColumn("partitionId",spark_partition_id()).groupBy("paritionId").count()
	display(df2)
	
======================================================================================================================================================
28.Interview Question - Shuffle Partition
	shuffle is required when you perform wide transformation like Agg,  joins where data is interdependent btwn the partitions.
	by default, shuffle partition parameter are set as 200.
	
	It means that when a shuffle operation is triggered (e.g., during a join or aggregation), Databricks will use 200 partitions by default to organize and distribute the data across the cluster.
	
	spark.conf.get("spark.sql.shuffle.partitions")
	
	we can also change it to our need - spark.conf.set("spark.sql.shuffle.partitions",<number>)
	IMP it is always good practice to maintain the partitions with size of 128MB to 200MB. and number of partitions are multiply of no of cores
======================================================================================================================================================
partitionBy and bucketBy are used while writing only:	
17.partitionBy(year,month) method is used to create directory-based partitions--create separate folder(year/month)
		
		We can also restrict the number records per file. for eg.
		
		df.write.format("csv").option("header",True).option("MaxRecordsPerFile",1000).partitionBy("Year").mode("overwrite").save("a/c/b")
		
Partitioning
Partitioning dividеs data basеd on thе valuеs of spеcific columns. This allows Spark to еfficiеntly filtеr and procеss data without having to rеad thе еntirе datasеt. It is particularly usеful for quеriеs that involvе filtеring on a particular column or pеrforming aggrеgations basеd on that column.

Example: Considеr a largе datasеt of wеb traffic logs that includеs columns for timеstamp, usеr ID, and pagе URL. You frеquеntly run quеriеs to analyzе traffic pattеrns for spеcific usеrs or pagеs. In this scеnario, partitioning thе data by usеr ID or pagе URL would significantly improvе quеry pеrformancе by allowing Spark to dirеctly accеss thе rеlеvant partitions without having to scan thе еntirе datasеt.

Whеn to Usе Partitioning:
You frеquеntly filtеr data basеd on a spеcific column.
You pеrform aggrеgations basеd on thе partitioning column.
You havе largе datasеts that nееd to bе procеssеd еfficiеntly.

======================================================================================================================================================
18.bucketBy(5,year,month).sort(product_name) --create 5 partitions based on hash value of bucketing value and sort it inside bucket.

	it will be useful when you perform join/agg, spark will understand the bucketing and it will significantly improve the performance.
	when you join any dataset, it will perform sort merge join automatically behind the scene. SMJ nothing but shuffling and sorting the data which are very 2 costly operations.
	And, whenever you call the action related these kind of wide transformation, it will perform SMJ everytime which is expensive.
	
	To avoid this, we can apply bucketing and keep the data shuffled and sorted ready.

	In partitionBy, it will create as many number of partitions(folders) as we select the partitioned column(usually low cardinality column such as country, department etc) means even if you have one record for that perticular partition it will create a separate folder for that.
	
	But in bucketBy it will created fixed number of partitions based on hash value of your mentioned column data.
	
Buckеting
Buckеting dividеs data into a fixеd numbеr of buckеts basеd on a hash function appliеd to a spеcific column. This tеchniquе is bеnеficial for opеrations likе joins and co-grouping, whеrе data from multiplе datasеts nееds to bе matchеd basеd on a common column. Buckеting еnsurеs that data with thе samе join kеy is distributеd еvеnly across thе buckеts, rеducing thе amount of data shuffling rеquirеd during procеssing.

Example: You havе two datasеts, onе containing customеr information and thе othеr containing ordеr dеtails. Both datasеts havе a column for customеr ID. You frеquеntly pеrform joins to combinе customеr data with ordеr information. Buckеting both datasеts by customеr ID would significantly improvе join pеrformancе by еnsuring that data with thе samе customеr ID is collocatеd, rеducing thе amount of data shuffling rеquirеd during thе join opеration.

Whеn to Usе Buckеting:
You frеquеntly join or co-group data from multiplе datasеts.
Thе join or co-grouping column has high cardinality (many distinct valuеs).
You want to minimizе data shuffling during joins. 
======================================================================================================================================================
19.Cache vs Persist
	Cache --will store results from transformation in memeory
	persist --it will store result in memory or disk
	
	we may get qn like, spark is already in memory computation, then why do we need cache and persist?
	use of cache, suppose some transformation of any df is repeatedly being called, then we can cache it instead calculating the logic again and again
	eg. df.cache()

	persist() :- Below are the params for persist()
	MEMORY_ONLY - stores the result only in memory. If size of the result is more than memory, only partial data will be cached and rest all would be re calculated
	MEMORY_ONLY_SER - same as above but serialized
	MEMORY_AND_DISK - stores partitions in disk those do not fit in memory
	MEMORY_AND_DISK_SER - same as above but serialized
	DISK_ONLY - Stores result in disk only.
	
	** if we put _2 like MEMORY_ONLY_2, it will create a replica so that we would not lose data if any of the node is failed
	**When the application completes successfully, Spark will automatically clean up the cached data, freeing up the resources
	
	NOTE:Here df.cache() and df.persist(MEMORY_ONLY)  are same.
	
	to cleanup data from disk --df.unpersist() --there is no concept called uncache--since cache is automatically cleared by spark
======================================================================================================================================================
30.Dataframe Checkpoint - Checkpoint is to store the intermediate result of any df into disk.
	for eg, if we have 10 df's and 6th and 7th df's are depending on df5, when you call df7 or df6 for any action,it will compute df1-df5 everytime
	to avoid that, we can do checkpoint on df5.
		like this -df1->df2->df3->df4->df5->df6
		like this -df1->df2->df3->df4->df5->df7
		like this -df1->df2->df3->df4->df5->df8
	
	It looks similar to cache but main difference is, in checkpoint it will truncate the DAG where as cache wont.
	****And, cache will store the result in memory where as checkpoint will always store in disk
		eg.# Enable checkpointing and specify the checkpoint directory
			spark.sparkContext.setCheckpointDir("hdfs:///user/checkpoint_dir")
			
			df.squared_rdd.checkpoint()
			
			since we are storing result in any reliable storage like HDFS, fault tolerance is guaranteed.
======================================================================================================================================================	
20. see point 22 - Catalyst Optimizer--in spark 2.0, in later versions lile spark 3.x onwards, they introduced Adaptive Query Execution
======================================================================================================================================================

21.Broadcast variable: you may get question--what is BROADCAST JOIN? belwo is the answer.
	in normal scenarios - when you submit the application, driver node will further partition it and send them across to all the worker nodes and it also furher create the stages and tasks. Task actually process the data at ground level.
	But, when you perform some joins b/t a huge table like fact table and small table like dimension table, data will be shuffled always btwn the nodes or workers which is costly operation. --THIS IS ALSO CALLED SHUFFLE JOIN
		to avoid this, we can make use of broadcast variable, to send complete copy of small table(in our case it is dimension table) to each worker node anfd it will perform join with fact table locally without data shuffling.BUT make sure your broadcast table should be smaller than your driver memory.
	
	syntax to make any dataframe as broadcast variable - broadcast(df), lets take our example, join fact table with dimension
	df1=[] --fact data
	df2=[] --dim data
			
		from pyspark.sql.functions import broadcast
		df3=df1.join(broadcast(df2),df1.id=df2=id,inner) --here you will not see join operation in DAG
		df3.explain(True) --to get the execution plan of any df --here True will give both physical and logical plan,without True only physical plan
		
		NOTE : If any dataset is less than 10MB, it would be apply broadcast join automatically
======================================================================================================================================================
22.Adaptive Query Execution:
	without AQE, means catalyst Optimizer--below is the execution flow:
	
{front end}	{Optimizer.....................................................................}					{Back end............................}
SQL API
DataFrame   -->Unresolved Logical Plan-->Logical Plan-->Optimized logical plan-->Physical plan **{cost model}**-> selected Physical plan-->execute RDD
Dataset 	

-->Unresolved Logical Plan = Checking syntax of code and semantics(checking if tables are avaialable if you are writing sql query)
-->Logical Plan = check for data types, Applying rule based(framing the query like from,where, group by etc) or cost based db engine
-->Optimized logical plan = find the most optimized plan for executing your code --if you have filter1 in stage 1 that would eliminate 10%data and filter2 in stage2 that eliminates 70% data, then it will apply stage 2 filter first to avoid carrry forward the unnecessary data.
-->physical plan = actual plan applicable for your code(can be n number of physical plans)

selected Physical Plan - out of n number of physical plans, it will choose the best and then apply to RDD.
	
Here once physical plan is selected, it will be applied to all stages in that job no matter what.

But, But, when AQE came into picture, it will re assess the execution plan right after completion of each stage and its output data.
By default, AQE will apply some optimization techniques such as broadcast join, COALSCE etc.

Question - When AQE is already taking care of broadcast join, why do we need to broadcast smaller dimension table explicitly?
Ans - To have more control on the execution

And also, if we have 5 paritions, and out of 5 , 3 are small and 2 are big. in normal scenarios spark will allocate them to execute them individually
and 2 big partitions will take time and rest 3 will complete soon and cores will go idle which is not good for performance.
So AQE, will merge smaller partitions and get them exected it in one core , means we are reducing the partitions, this concept is also called
COALSCE partitionining

To enable the AQE - spark.conf.set("spark.sql.adaptive.enabled",True) --However,AQE is enabled by default from Apache Spark 3.2.0

SHUFFLE join is not good as it is costly. ----AQE comes handy
Broadcast join is fine but has limitations like only small dataset/dataframe can be broadcasted-----AQE comes handy
Data Skew -- un evenly distributed data, i.e bad partitioning--means, suppose if you have sales data, and for this sales data, 80% comng from one country and rest all 20% coming from other 10 countries, we do partitioning based on country then 80% data will go into one executor which is bad

In this case AQE- automatically detects data skew.

======================================================================================================================================================
27.How to handle data skewness(un evenly distributed data)
	1. enable AQE  --see above
	
	IMP:from version 3.x above, Enabling AQE, will take care of salting,broadcast joining and skew hint automatically
	
	2. use broadcast join --see above	
	3.salting--splitting the big partition into further smaller partitions and name them with psuedo columns.
	4.skew hint --
		df = spark.sql("SELECT * FROM mytable WHERE id > 100 SKEWED BY id")--here we are telling spark that df is skewed based on id, so that wll apply optimization techniques automatically.
		
======================================================================================================================================================
29. Autoscaling - min - max worker nodes
======================================================================================================================================================

68.Pyspark | Performance Optimization: Delta Cache
	As we know, when we create delta table with path, it would create the table/files in specific location, can be DBFS,HDFS or Azure ADLS or S3.
	suppose, If I am doing some analytics, in that case whenever I query this delta table, it will always read from above remote location.And it will create the latency and hit the performance.
	
	To overcome this, we can cache the table into local disk of node(every worker node has RAM memeory and disk as well incase of, dataset is not going to fit in memeory there will be spill over) and whenever you query table it will actually fetch from delta cache instead remote location.
	
	by default, delta cache would be disabled, to check - spark.conf.get("spark.databricks.io.cache.enabled")
	
	enable delta cache - spark.conf.set("spark.databricks.io.cache.enabled",True)
	
	cache select * from delta_table_name --In snowflake it is Result Cache.

################################################### END OF PEFORMANCE OPTIMIZATION ###################################################################

==>Identify and filter out the records based on null values.
23.isNull and isNotNull
	df2=df1.filter(df1.emp_name.isNull())
	df3=df1.filter(df1.emp_name.isNotNull())
	
	if you want to add one or more conditions -
		df4=df1.filter((df1.emp_name.isNotNull()) & (df1.salary>3000 --& for and , | for or
		**you need to maintain () between conditions else it will throw an error.
	
======================================================================================================================================================
24.Identify and drop records having nulls(null can be in any of the column)--parameter is important here.
	
	syntax:df.dropna(parameter) --parameter can be -"any" --it will drop the records if it is having null in any of the column
											       -"all" --it will drop the record if is having nulls in all columns
											   [subset=col1,col2..] --it will drop the record if it is having nulls in mentioned columns
											   df5=df1.dropna(subset=["emp_name","salary"]) --drop only if these 2 columsn are null
											   here default is any
======================================================================================================================================================											   
25.Identify nulls and fill with some values
	syntax: df.fillna(value='some value')  --we give value=any number, it will replace only numbered columns.
										--if we give string value(value="somethig"), it will replace only string columns
										NOTE that it will replace whole dataframe
										
								if you want to fill in certain columns only, then use below syntax			
			df.fillna(value='some value',subset=["col1","col2"..])

======================================================================================================================================================
26.UDF -user defined Function
		eg.
		
		defining the funtion -
		
		def fun_name():
			some operation
			return val
	Here UDF are blackbox for spark, means spark will not apply any performance optimization techniques for UDF written in python or scala
	because, spark will execute the code in JVM(java machine)
	and, UDF are mainly to build custom logic, hence spark treat UDF as one unit of computation without knowledge of its complete behaviour
	
	While UDFs may not be as optimized as built-in Spark operations, they provide flexibility to perform a wide range of transformations and computations on your data. It's important to carefully design and use UDFs to balance flexibility and performance in your Spark applications
======================================================================================================================================================
31.Compression Methods: Snappy vs Gzip --these are the widely used compression methods in bigdata projects.
	when you save files, the default compression type is snappy
	
								Snappy 								vs 										Gzip
						low CPU utilization															high CPU utilization
						low compression rate														high compression rate
						i.e converts 1GB to some 500MB												i.e converts 1GB to some 200MB
						splittable																	non splittable
						Use case - Hot layer														Use case - cold layer
						Use case - Compute Intensive												Use case - Storage(archive) Intensive
						
						
df.write \
  .format("parquet") \																				
  .option("compression","snappy") \  ##"gzip"
  .save("FileStore/spark/test/compressionFiles")
  
======================================================================================================================================================
32. PySpark Functions

	split: split("string to be splitted","on what basis t split space,-,; etc".getItem(0 or 1 etc)
	0--first splitted values, 1 second splitted value --so on
	
	df=df.withColumn("first name",split("name","-").getItem(0))
	df=df.withColumn("last name",split("name","-").getItem(1))
======================================================================================================================================================
33. Arrays_zip
	to map elements avaialable in different arrays.
	
	("ABC Inc.", [101, 102, 103], ["HR", "Finance"]
	
	from pyspark.sql.functions import arrays_zip,explode
df_zipped=df.withColumn("zipped_array",arrays_zip("Employee IDs","Department Names"))
	
	o/p-
[{"Employee_IDs": 101, "Department_Names": "HR"}, {"Employee_IDs": 102, "Department_Names": "Finance"}, {"Employee_IDs": 103, "Department_Names": null}]

Once mapped, we can flatten it using explode

======================================================================================================================================================
34.Array_Intersect
	get you common elements from 2 arrays without duplicates
	
	intersect("array1","array2")
	
	df=df.withColumn("intersect",intersect("array1","array2"))
	
	array_union--union arrays removing duplicates
	array_except --print from array1 that are not avaialable in array2
	array_distinct --print distinct values from array1
	array_min --print min value in from array
	array_max --prnt max value in array
	array_sort --sort the array --default asc
	 to desc ---> .desc()
======================================================================================================================================================
35. Concat --to concatenate multiplecolumns/strings into one string(no delimeter)--Concat("a","b") =>ab
	Concat_ws --same as Concat but with delimeter --Concat_ws(",","a","b") =>a,b/t
	array_join - to concatenate the elements in a array - array_join(["a","b","c","d"],",") =>a,b,c,d
	
======================================================================================================================================================
36.Pyspark | Null Count of Each Column in Dataframe
	from pyspark.sql.functions import col,count,when

	cols=df1.columns  #['emp_id', 'emp_name', 'salary', 'location']

	df=df1.select([count(when(col(c).isNull(),c)).alias(c) for c in cols])

	display(df)
======================================================================================================================================================
37. Pyspark: Find Top or Bottom N Rows per Group
	Almost same as window functions in SQL
======================================================================================================================================================	
38. while reading a file, when you dont mention anythin about schema --then only 1 spark job is created to get the no of columns
						  when you infer schema --2 jobs(1 for get the no of columns, 1 for their data types)
						  when you create schema explicitly and attach it --then no sparks job created
======================================================================================================================================================	
39. Greatest and Least --Greatest - max of row level
						Least --min of row level
						**these 2 give result in row level
						df_greatest=greatest("Math","English","Science","History","Geography")
						df_least=least("Math","English","Science","History","Geography")						

	Max and Min			Max -max of column level
						Min - Min of column level
						**these two in agg level
						df_maths_max=df.agg({"Math" : "max","English" : "min"})

						
######################################################################################################################################################
DELTA LAKE:

What is lakehouse?
In short, a Data Lakehouse is an architecture that enables efficient and secure Artificial Intelligence (AI) and Business Intelligence (BI) directly on vast amounts of data stored in Data Lakes.
						
40. DWH - can support only structured data --DML is easy
			But can not support scaling up/down and cannot support semi structured or un structured data
to overcome above challenges, introduced datalake --it supports all types of data(structured,semi structured,un structured)
	but at the same time, it has created new challenges like 
		it wont support any dml operations since data will be stored in raws format, so identifying any record and performing DML is very costly
		If we are doing some insert operation in datalake, if system failes it will leave the data in corrupted state
		it wont support ACID properties
	
So here comes the delta lake --combination of best features/elements from datawarehouse and best features/elements from datalake in addition to time travel.
	and it also support streaming data.
	
	So delta lake nothing but a storage layer sitting on top of datalake. So datalake contains all the raw data related to bigdata in csv,parquet and json formats. And, delta lake holds all the metadata information (like column name, types,lengths, other stats) so that we can perform DML easily.
	
	DELTA lake is highliy scalable,reliable(doesn't leave the data corrupted) and securable.
	
	
41.DELTA LAKE Internal mechanism:
	When you create a table in delta lake, it will actually store metadata information in json log file level and  actual data will be stored in storage layer in parquet format (path will be provided)
	for every dml operation(insert,update,delete,merge), it will create a log file in the form of JSON and for every 10 log files it will create a checkpoint file in the form of Parquet file.
	
	In delta log folder, alongside json, it will also create .crc(cyclic reduntancy check) typically only a few bytes in size. The checksum is calculated using the CRC32 algorithm. 
	
	The .crc file is used by Delta Lake to ensure that the data in the transaction log is not corrupted. If the .crc file does not match the .json file, then the data in the .json file is considered to be corrupted and will not be read by Delta Lake. 
	
	The .crc file is also used by Delta Lake to optimize queries. The checksums can be used to quickly determine which files in the transaction log need to be read in order to satisfy a query. This can significantly improve the performance of queries on large Delta Lake tables.
	
	Checkpoint file will be useful when we do time travel, means spark doesn't necessarly need to scan all the files rather it can check checkpoint file.
	
	The default retention period of any data file(when we perform delete operation) is 7 days(it can be changed according to our need) means when you delete some data, data file will not be deleteted immediately rather it will do soft delete.means it will mark the record as delete in json log file. It means we can do time travel 7 days by default.
	
	***even after 7 days, files will NOT be deleted physically and will not be visible for time travel.
	
	but we can change it to desired number - 
	spark.sql(
  """
    SET delta.deletedFileRetentionDuration = "30 days"
  """
)
	
	Note: if we are increasing the retention time period, it will occuly more storage
	
	**while creating the delta table, we usually give the path where to store parquet data files to have more controls
	Incase, if we dont specify the path , it will create the data files and delta log files in default location(dbfs:/user/hive/warehouse/delta_employee_without_location)
	
	For every DML operation on delta table it also maintain the versionining
	
42. Managed table:
	databricks will manage both metadata and data files for you, this is the recommended approach in order to achieve time travel,ACID and security features of delta lake.
	
	Un managed table or External Table:
	if you would like databricks to manage only metadata and you want to store your data files in someothe location(like S3 or GCP storage).
	in this way end user has more control on the data. However in this case also time travel,ACID possible.
	
Differece between managed and unmanaged tables(also called external table) is location of the files.

-- Replace the placeholders with your actual values
CREATE TABLE your_external_table
USING parquet
OPTIONS (
  'path' 'abfss://your_storage_account@your_container.dfs.core.windows.net/path/to/parquet',
  'fs.azure.account.key.your_storage_account.blob.core.windows.net' 'your_storage_account_key'
);

	
43. Data Lake Solution Architecture:
	source -->RAW(data lake) -->Curated(data lake)--------------------------------->Presentation/serving layer(data lake)
								basic transaction like cleansing nulls,				aggregated data/ready to use
								remove duplicates etc
	
	**here all three layers are in datalake, in data lake architecture, we can not achieve any DMLs or ACID
	**-->means any ETL tool like ADF, ADB, AWS Glue, GCP data flow etc.
	
	to over come this -

44. Delta Lake Solution Architecture:
	source -->Bronze(data lake) -->Silver(delta lake)--------------------------------->Gold/serving layer(delta lake)
								basic transaction like cleansing nulls,				aggregated data/ready to use
								remove duplicates etc
	**here Bronze layer is in datalake since it is just to maintain the history data.And this approach recommended by databricks

45. standard/mostly used ways to create delta table		
			
		Usin Pyspark method
			from delta.tables import  *
		1.DeltaTable.create(spark) \  ---createIfNotExists --createOrReplace
			.tableName("tab_name") \
			.addColumn("emp_id","INT") \
			.
			.property("some descritpion") \
			.location("some path")
			.execute()
		
		2.SQL method:		--IF NOT EXISTS --create or replace
		
			create table emp(
				emp id int,
				.
				.
				)using DELTA
				location("some path")
		3. from DF
			spark.write.format("delta").option("path","some location").saveAsTable("tab name")
			
46.Pyspark| Delta Lake: Delta Table Instance
	for example, if we have to perform any dml operation using pyspark(not spark SQL) then we can create a instance/replica of delta table and we can do DML operation on that instance. it is like working on cloned table. It will still reflect the base table.
	
	syntax 1:instanc1=DeltaTable.forPath(spark,"path to delta table")
			 instanc2=DeltaTable.forName(spark,"delta table name")
			 
			 after creating instance you can fire DML operations using pyspark on that instance
			 
			 it is like taking snapshots of detla table at any given point. This will be helpful for time travel and data recovery.
			 
47.Different Approaches to Insert Data Into Delta Table
	SQL style insert:
	%sql
	insert into ...
	
	usinf df:
	df1.write.insertInto("tab_name",overwrite=False)
	
	spark SQL:
	spark.sql("insert into ..")

48. Different Approaches to Delete Data from Delta Table
	
	SQL method:
	
	%sql
	delete from emp where ..
	
	using delta location:
	delete from delta.location..  mot commonly used
	
	spark sql:
	spark.sql("delete from ..")
	
49.Delta Lake : Update Delta Table
	same as delete

50. SCD Type -1 using merge statement --check notebook

51. to maintain the audit on delta table
	--first get the history of delta table by creating a instance on that delta table
	
	from delta.tables import *
	df_instance=DeltaTable.forName(spark,"delta_emp")
	lastOp=df_instance.history(1)
	
	you can explod perticular metric column and insert data into audit table
	we can also have other audit columns like user name, notebook name, timestamp etc 
	
	check the notebook for more details

52.Delta Lake : Slowly Changing Dimension (SCD Type2)
	refer notebook--preferrably merge statement
	
53.Time travel(back and forth) on delta table
	make use of timestamp as of and version as of
	
	in SQL --select * from tab timestamp as of <sometime> --you can get timestamp from describe history tab
	in pyspark --df=spark.read.option("timestampAsOf","sometime").table("tab")
	
	in sql - select * from tab version as of <version number> --you can get version from describe history tab
	in pyspark --df=spark.read.option("versioAsOf","version number").table("tab")

54. Pyspark| Delta Lake: Restore Command
	once we do the time travel and if we decided to restore to any previous version then use restoreToVersion/restoreToTimestamp
	
55. Pyspark | Delta Lake: Optimize Command - File Compaction
	As we know, whenever we perform any dml operation on delta table, it will always create new file and it will keep on increasing these files as we proceed with several dmls.
	at some point, it will be overhead for spark to maintain the metadata of all these files
	that's where optimize comes into picture.
	
	usually optimize command combines smaller files into 1GB size file. It does not mean it will remove the files, but going forward it will refer the latest optimized(combined) file instead of all files.
	
56.Pyspark | Delta Lake: Vacuum Command

	I am inserting 4 records and it has created 4 data parquet files 
	if I delete record -1 then that perticular file be made as invalid(It will NOT be deleted physically)**No new file will be created in this case
	If I update record -2 then that perticular file be made as invalid(It will NOT be deleted physically) and will create 2 new files.
	if I insert new record, new file will be created
	
	
	After some point I also ran optimize command that is optimize delta_tab_name(it will combine all smaller files in large file of <=1GB), POINT to note, when we optimize the delta table, it will create new optimized file(delta will refer this file only going forward) and make all related smaller files as invalid.
	
	In this way also , invalid files keep increasing
	
	Like wise , if we keep doing/performing DML operations, it will keep building invalid files
	and at some point we will have to clean them to save storage and cost and also reduce the over head of maintaning metadata
	
	
	To do this, we have Vacuum command. this command will only remove invalid files(retention days we have to mention).
	BUT BUT , always perform dry run before running actual vacuum
	
	VACUUM delta_tab_name DRY RUN
		It will list down the invalid files are ready for deletion
		
		--delete files older than certain days(need to give hours in command)
		VACUUM delta_tab_name RETAIN 720 HOURS DRY RUN
	
		--delete all invalid files from now
		VACUUM delta_tab_name RETAIN 0 HOURS DRY RUN --it will through error --mechanism given by dataricks
		
		BUT to allow this kind of deletion, you can make retentionduration check as false usinf below command
		set spark.databricks.delta.retentionDurationCheck.enabled = False
		Now VACUUM delta_tab_name RETAIN 0 HOURS DRY RUN will work
		
		
		
		so VACUUM delta_tab_name fianlly invalid files that are invalidated older than 7 days by default
		***after vacuum, we can not do time travel
		
57.Pyspark | Delta: Z-Order Command
	z order is the extension to optimize. as we know when you optimize, it will combine smaller files into one file(~1GB file) and make all these smaller files as inactive. 
	for eg, you have 100s of smaller files and it optimized all of them into some 10 files. And all these 10 files are not in sorted order.
	Now, when you perform some analytical activities like filter, agg or join, we may end up with scanning unnecessary files out of these 10 which will hamper the performance
	
	here comes z order comes into picture. When you perform optimize alongside z order , it will sort the data and create optimized files.
	this way, when you do analytics , it will only scan required files.
	
	%sql
optimize delta_emp_type2 zorder by (emp_id)
	
58.Pypark | Delta: Schema Evolution - MergeSchema
in many big data projects, schema from source file/data may change over a period of time.
	eg, are delta table is created based on initial schema like(id,name,salary) and after few months we started receiving one more additional column called dept.
	And, when we try to write data into our delta table, it will fail due to schema mismatch. To handle this scenario schema evolution comes into picture.add option("SchemaMerge",True) while writing into delta table and it will create the newly coming column automatically in delta table.
	
	Here note that, it will fill as null for for this new column for old records.
	
	**And, when some column is removed at source file end, then also schem merger will handle it BUT BUT it will not drop the this perticular column from delta table. Instead it will keep addition as null to this deleted column.
	
	Pypark | Delta: Schema Evolution - OverwriteSchema
		It will drop or rename the column from delta table according to source system.
	
59. Ways to insert the data into delta table:
	1.df.write.InsertInto("table_name",overwrite=False)
	
	2.df.write.mode("append").saveAsTable("tabname")
	
	3. create view and insert into table--insert into tab select * from view

60.Pyspark | Data Skewness| Interview Question: SPARK_PARTITION_ID
	when we read external file, spark will do the partitioning by default like filesize/128MB,
	
	if you would like to know about each partition's id, we can use spark inbuilt function called SPARK_PARTITION_ID.
	
	eg. df.withColumn("part_id",SPARK_PARTITION_ID())
	
61.Pyspark| Input_File_Name: Identify Input File Name of Corrupt
	from pyspark.sql.functions import input_file_name
	
	df.withColumn("src_file_name",input_file_name()) --will give you input file name against each record
	
62.Pyspark | Window Functions: Lead and Lag
	to get lead and lag of record partitioned by any column ---just like in SQL

63. check file/folder exist or not in dbfs

64.Pyspark | Interview Question: Sort-Merge Join (SMJ)
	sort merge join is the internal mechanism, when you perform any kind of join(inner,left,right,full) SMJ will trigger automatically behind the scene
	
	once partition is done with source data.
	flow is --shuffle the data b.w executors(spark will decide the suffle looking at df's) -->sort -->merge it.
	
65.explain plan
		df.explainPlan() --will give you the execution plan how it is executing behid the scene.
		by default it will give you physical plan
		if you need to know all plans like unresolved logical,logical and physical-then pass one parameter called "extended"
		if you need to know the plans in well formatted human readable--then pass "format"
		
66. Pyspark:Interview Question|Scenario Based|Max Over () Get Max value of Duplicate Data
	1.get max price and max discount using winodw function --partionBy(id)
			newcol1=max(price).over(partionBy(id))
			newcol2=max(discount).over(partionBy(id))

67.Pyspark | Create_map(): Convert Dataframe Columns to Dictionary (Map Type)

69.Create Azure keyvault and create secrets and also values
	create secret scope in databricks-> put #secrets/CreateSecrete in url
	give DNS name and resuource ID in databricks --to communicate with azure keyvault
	**to retreive secrets, dbutils.secrets.listScopes()
	
70.Widgets can be created in databricks, to avoid hard coding any values like, table name,schema,db name etc.
	dbutils.setwidgets.text(""widget_name","") --"" for default value
	We can pass above required parameters from ADF

71.in vise versa, we can also pass the values(o/p values back to caller like ADF) using dbutils.notebook.exit("value")
	in ADF - activity to run ADB notebook -> set variable
											in set variable-->@activity('activity name').output.runOutput

72.**ADF storage event trigger -->when ever there is new file arrived in ADLS, it will activate the pipeline and trigger the adb notebook
	***in notebook, to pick only the latest file, you can sort the file list in desc and pick only the latest file [0]
	
*******inteview qn	
73.Read Excel File with Multiple Sheets
	df=spark.read.format("com.crealytics.spark.excel").option("inferschema",True).option("header",True).option("dataAddress","sheet2!").load("path/to/file")
	here we need to install com.crealytics.spark.excel from maven at cluster level
	
	**here is the tricky part. above code can read only one sheet from excel at a time. but what if we want to read from all sheets
	to do that, we can write UDF
	
	sheet=['sheet1','sheet2','sheet3']  ##we can also take the sheets dynamically if we dont how many sheets will be there in excel
	path=a/b/c/
	
	def excelDF(path,sheets):
		firstSheet=sheets[0]
		df=spark.read.format("com.crealytics.spark.excel").option("inferschema",True).option("header",True).option("dataAddress",{firstSheet}!).load(path)
		
		for sheet in sheets[1:] ##starting from second sheet
			sheetDF=spark.read.format("com.crealytics.spark.excel").option("inferschema",True).option("header",True).option("dataAddress",{firstSheet}!).load(path)
			
			df=df.union(sheetDF)
			return df
			
74.Handlining Duplicate Data: DropDuplicates vs Distinct

75. distinct vs dropDuplciates()
	both are same unless we give any columns as parameters ti dropDuplciates to delete duplicates only based on certain columns
	
	df.distinct() --delete duplicates based on all columns
	df.dropDuplicates() --delete duplicates based on all columns
	df.dropDuplicates("id") --will delete duplicates based on id column -->print all columns
	
	work around for distinct to delete bases on certain columns-->
		df.select("id","name").distinct() --but it will print only id and name

76.Performance Optimization: Select vs WithColumn
	df.withColumn(col1,operation)-->will create new column and new df --performance impact
	
	so better go with select, eg: df.select("*",concat(col("firstname")),lit(" "),col("lastname")).alias("full name")
	
	both produce same result but select wins the race in terms of performance
	
77.StructType and StructField are used to define a schema for dataframe
	
	eg. df_sch=StructType([
		StructField("id",IntegerType(),nullable=True),
		StructField("name",StringType(),nullable=True)
	])
	
	**we can also nested the any field like below:
	df_sch=StructType([
		StructField("id",IntegerType(),nullable=True),
		StructField("name",
			StructType([
			StructField("firstName",StringType(),nullable=True),
			StructField("middleName",StringType(),nullable=True),
			StructField("lastName",StringType(),nullable=True)
		]))
	])

**here the drawback is, all fields/StructField should have the data, if not it will throw an error.
		eg, if we are not passing middleName for any record, then it will through an error while creating df
		you will have to fill None/null at least
		
		to overcome this, we can go with map()

78.Schema Definition: Struct Type vs Map Type		
	
		df_sch=StructType([
		StructField("id",IntegerType(),nullable=True),
		StructField("name",
			MapType(StringType(),StringType(),True)
			)
	])
	
	data should be provided in json format --{"firstName":"a","middleName":"b","lastName":"c"} --here any of these can be null
												{"firstName":"x","lastName":"z"} --it will not throw error.


79.**if your requirement is, if any of the column is expecting array type values, then you can go with ArrayTpe()
			eg.StructField("Hobbies",ArrayTpe(StringType()) -->it will expect list of values


80. Schema Comparison b/w df's
	if df1.schema == df2.schema
		print("schema matches for both df's")
	else:
		print("schema does not matches for both df's")
	
	**if schema does not matches, I want to know what all are the columns missing or extra in each df
		lsit1=list(set(df1.columns) - set(df2.columns))
		

	
81.to encrypt data or any column -->we use encrypt function but prior to this, we need to install fernet library
		from cryptography.fernet import fernet
		key=fernet.generate_key()
		f=fernet(key)
		
		data="abc"
		enc_data=f.encrypt(data)
		
82.Interview Question: Pyspark VS Pandas
	
		Pyspark																			Pandas
		open source Python lib ,written in Scala									open source python lib, written in Pythan
		mainly for big data analytics and distributed data processing				does not support distributed data processing, runs on single node
		df's are immutable															df's are mutable
		support parallel processing													does not support parallel processing
		Lazy evaluation																Eagarly evaluation
		
	**Python community introduced polars library recently. It is almost same as pyspark df
		
83.array_repeat("col_name",10) --it will create  list of data 10 times of col_name, later we can explode it	
		this will be used to create test data
		
		df.withColumn("new_col",array_repeat("id",100))
		
		explode("new_col")
		
		
84. subtract and exceptAll -->I want to read data from df A, records that are not avaialable in df B
		difference b/w subtract and exceptAll is, subtract will NOT keep duplicates where as exceptAll keep them
		
		df1.subtract(df2)
		df1.exceptAll(df2)

85. LAST and FIRST window fucntions
	LAST --last record data from partition or window
	FIRST --first record data from partition or window
	
	just like lead and lag
	
86.	skip first N records from a file
	use .option("skipRows",10) --it will ski first 10 rows and create df

87.But how to skip certain range?? eg:skip rows from 10 to 20
	fulldf=full read
	df1= spark.read.option("skipRows",10).load(path) -->will give you all from 10 records
	df2=fulldf.subtract(df1) -->first 10 records
	df3=spark.read.option("skipRows",20).load(path)--gives you all records from 20
	
	now union df2 and df3
	
88.from spark 3.4 onwards, we can directly query the dataframe. Earlier we used to create a view/TempView/table then used to query it
	res=spark.sql("select * from {df_tab}",df_tab=df)

******
89.Performance Optimization: Re-order Columns in Delta Table
	why reordering of columns is very important in delta lake. Ideally when you create any delta table and start loading data into it, it will create data file in format of parquet and statistics in json format. But it will only capture first 32 columns of delta table in json file and it will apply optimization and data skipping techniques based on these 32 cols.
	
	Suppose, we have more 32 columns liks some 100, and most of the columns that are being using in data analytics are after 32, in that case it will hamper the performance
	
	In this scenario, we need to re order the columns and keep most used columns in first 32.
	
	eg: ALTER TABLE emp CHANGE COLUMN COL35 FIRST --will re order col35 to first place
	ALTER TABLE emp CHANGE COLUMN COL35 AFTER COL10 --will put col35 after col10
	
	now, if you open log file(json file) , you will see aboe column col35 in the statistics. This way we ca re arrange the columns
	**It will insert the data in proper place only when we load some more records after altering it since we will load based on schema.
	
90. Types of cloning in databricks
	Deep clone -- will copy the data physically and time consuming, mostly used for backing up the tables
	shallow clone -- only metadata will be copied and no physical data

91. types of views:
	normal view - persisted across databricks
				-can be dropped manually
				
	temp view - for session only (notebook level,when you install python library, when you restart cluster)
			-will be dropped when session ends
			-They are not persisted in Unity Catalog and are automatically dropped when the session ends.
			-You cannot directly access temporary views from Unity Catalog itself. However, you can use them within your SQL queries or notebooks to access the underlying data they represent.
			
	Global temp view - at cluster level.(will be avaialable for all who are using same cluster)
			-will be dropped when cluster restarts
			-They are stored in the global temporary schema within Unity Catalog, allowing access from any session in the workspace.
			-You can view and manage global temporary views directly from Unity Catalog using SQL commands like SHOW VIEWS and DROP VIEW.

92. We can directly read data from files
	select * from file_format."path"
	eg. select * from json.`a/bc/file1.json`
	
93. To parse the json data:
# Sample JSON data
		json_data = [
			('{"name": "John", "age": 30}'),
			('{"name": "Alice", "age": 25}')
		]

		# Create a DataFrame with JSON column
		df = spark.createDataFrame(json_data, ["json_column"])

		# Define the JSON schema
		json_schema = StructType([
			StructField("name", StringType(), True),
			StructField("age", IntegerType(), True)
		])

		# Parse the JSON column using the schema and extract fields
		parsed_df = df.select(from_json(col("json_column"), json_schema).alias("data")).select("data.*")

		# Show the results
		parsed_df.show()
		
94. The explode function in PySpark is used to split an array or map column into multiple rows, thereby "exploding" the column. Here's an example
		 Sample data with an array column
		data = [
			(1, ["apple", "orange"]),
			(2, ["banana", "grape"])
		]

		# Create a DataFrame
		df = spark.createDataFrame(data, ["id", "fruits"])

		# Explode the array column into separate rows
		exploded_df = df.select("id", explode("fruits").alias("fruit"))

		# Show the results
		exploded_df.show()
		
95. Structed streaming:
	read:
		streamDF=spark.readStream \
					  .table("")
		
	write:
		streamDF.writeStream \
			.trigger(ProcessingTime = "5 minutes") \
			.outputMode("append") \
			.option("checkpointLocation" = "/path")
			.option("table name")
			
			**here trigger(ProcessingTime = "5 minutes") --to check the source df for every 5 min, if we dont specify any time interval it will scan source df for every 30sec.
			
			**trigger(once = True) --load all avaialable data for one time and then ***stop***
			**trigger(AvailableNow = True) --load all avaialable data in micro batches and then ***stop***
			
			**outputMode("append") --append newly coming records
			**outputMode("complete") --overwrite the target table every time
			
			**.option("checkpointLocation" = "/path") --to track the progress
			**it will gurantee the data is loaded exactly once and also guaranteed the fault tolerance
			
	**short coming of streaming data frames
		- sorting is not possible
		- deduplication is not possible
		
96. Auto loader is designed based on above spark stream concept
		read:
		streamDF=spark.readStream \
					  .format("CloudFiles") \
					  .option("CloudFiles.format","csv") \
					  .load("file path")
		
	write:
		streamDF.writeStream \
			.trigger(ProcessingTime = "5 minutes") \
			.outputMode("append") \
			.option("checkpointLocation" = "/path")
			.option("table name")
			
97. We can also use COPY INTO command in SQL eg. copy into table_name from "file path"

	but when to use copy into and when to use auto loader
		copy into											auto loader
		thousands of files									millions of files
		less efficient at scale								more efficient at scale
		
98. multihop architecture or medallion architecture
	S3-bronze-silver-gold
	
	S3-bronze:
				read:
		streamDF=spark.readStream \
					  .format("CloudFiles") \
					  .option("CloudFiles.format","csv") \
					  .load("file path")

			write:
		streamDF.writeStream \
			.trigger(ProcessingTime = "5 minutes") \
			.outputMode("append") \
			.option("checkpointLocation" = "/path")
			.option("table name")		
			

	bronze-silver(cleansing, type casting or other new columns such as input_file_name)		
	spark.readStream \
		.table("bronze table") \
		.createOrReplace Temp View("view name")
		
	silver to gold:
		
		create a temp view(something_temp_v) using some aggregations on silver view(count(books) etc)
		
		and write into gold layer using above temp v
		spark.table("something_temp_v") \
			.writeStream \
			.outputMode("append") \
			.option("checkpointLocation" = "/path")
			.option("table name")
=====================================================================================================================================================
		
99. Databricks | Pyspark| AutoLoader: Incremental Data Load
--To load incremental load (only new or modified records)
Characteristics of Incremental Load:
	1.Only process new data files
	2.process the data load asap new data detected
	3.Dont miss any new files
	4.Good performance while processing larger directories
	5.Repeatable pattern
	
Incremental loading patterns:
pattern -1:
	-->Water mark method (storing timestamp somewhere for comparison)
	-->Program will keep running to check if new files arrived
	
	Short coming of above pattern -> Pipeline has to be executed as per schedule even through there is no new data or new file detected
								  ->Incur unnecessary cost
pattern -2:
	-->checkpointing
	-->Peformance bottleneck while listing larger directories where millions of files to process

what is the best solution then?
-->AutoLoader (Continuously,efficiently,automatically loading new files as they arrive)

What is AutoLoader:
	Incremental Data Load: Feature provided by Databricks to process incremental data load efficiently and automatically.
	Cloud Storage System: Currently supporting only ADLS, S3, GCP and DBFS
	File formats: Supporting bigdata file formats JSON,CSV,Parquet,AVRO,ORC,Text and Binary file formats
	
Components of AutoLoader:
	->Storing the metadata of processed files through internal database(RocksDB)
	->Processing new files immediately using spark streaming
	->Using advanced optimized Cloud - Native components to identify new files as soon as they arrive

Two parts of AutoLoader:
	Cloud Files Data Reader + Cloud Notification Services = AutoLoader

File detection mode:
1.Directory listing mode
2.File notification mode

By default, if we dont specify, it is Directory listing mode, but,File notification mode is much faster. However, for File notification mode we need to speicify certain properties of storage service like client id, secret id etc.

Basic configuration:
		streamDF=spark.readStream \
					  .format("CloudFiles") \ --this will identify as a autoloader
					  .option("CloudFiles.format","csv") \
					  .load("file path")

Here we did not mention anything about file detection, hence it is Directory listing mode

Use notifications:
		streamDF=spark.readStream \
					  .format("CloudFiles") \ --this will identify as a autoloader
					  .option("CloudFiles.useNotifications",True) \
					  .option("CloudFiles.format","csv") \
					  .load("file path")
					  
Alongwith, useNotifications option, we also have to mention service principal (subscription id,connection string,tenantId,clientId,clientSecret,resourceGroup)

eg:
CloudFileConfig = {
ubscription id ="",
connection string="",
tenantId,clientId="",
clientSecret="",
resourceGroup=""
}

		streamDF=spark.readStream \
					  .format("CloudFiles") \ --this will identify as a autoloader
					  .options(**CloudFileConfig) \ ---** since it is python dictionary, usually it is * for array/list
					  .option("CloudFiles.format","csv") \
					  .load("file path")

to Write:
streamDF.writeStream \
		.trigger(once=True) --for full load, we have processingTime="1min etc"
		.save("table location")
		
=====================================================================================================================================================
100.Databricks | Pyspark| Delta Live Table: Introduction

DLT is a hot topic nowadays. Many oranizations are started implementing their projects on DLT.

We have medallion architecture lakehouse -> where we ingest data into Bronze layer with help of any ETL service/tool and from Bronze to Silver applying some basic transformations like type casting, adjusting date formats, cleaning up the duplicates etc. and from siler to Gold layer
by applying all the business logics and aggregations.

Looks simple isn't it? --Nope
In real time you will see lot of complex logic between each layer it is a challenge to implement using medallion architecture.

Below are the challenges in medallion architecture:
1.Troubleshooting and debugging is a challening and time consuming
2.Data quality is poor and need to handle manually
3.Data linege is not avaialable to monitor the pipeline visually. Everytime we need to open the code and check like what is dependency for this table where it is inserting etc.
4.Visibility and monitoring at more granular and logic level is not avaialable.
5.challenge to combine both streaming data and batch data in a single pipeline.
eg.In today's bigdata world, we are dealing with both streaming and bactch data. In a situation we may need to load streaming data into fact table and batch data into dimension table(as we know dimension tables wont get changes frequently).
This is quite complex to handle in medallion architecture.

****Below are the steps generally involved to build a any data pipeline.
.Code development  .cluster management .Data quality checks .infrastructure management .dependency management .fault tolerance .data governance .Prod

But But, using DLT we can simply focus on ONLY Code development and rest all can be taken care by DLT.

So what is DLT? 
DLT is a development framework privided by databricks which simplifies and accelerates ETL pipeline building process
DLT follows declarative approach instead of procedural approach
DLT framework automatically manages the infrastructure scale.
With the help of DLT, developers can spend more time on development rather than tooling and infrastructuring.

Below are the greate functionalities for DLT:(that will do automatically for you)
1.Data transformation
2.Cluster management
3.Data quality validation
4.Accelerate development process
5.Task orchestration
6.Monitoring
7.Error Handling
8.Unified batch / streaming process

Development steps involved:
1.Create live tables using notebook
2.Createa DLT pipeline using workflow
3.Run/schedule the pipeline
4.Monitor metrics in UI

Differece between procedural approach and declarative approach:
Procedural approach:
1.Explicitly define all ETL steps(ingesion,transformation and loading)
2.must follow list of steps with proper order
3.Execution steps must follow some procedural steps. Need to dictate each step "HOW" to perform
4.extensive code generation
5.Time consuming and error prone
6.challenge in debugging and error handling

Declarative approach: --It is completely opposite to procedural approach.
1.All ETL steps are abstracted
2.No specific order to be followed.
3.You just declare "WHAT" is needed
4.Inteleegent engine will automatically generate the code to extract, transform and load
5.less code
6.Debugging and error handling is easy

**It is like outsourcing procedural approach to someone**

=====================================================================================================================================================
101. Databricks | Pyspark| Delta Live Table: Datasets - Tables and Views

IMP - Currently DLT supports only Pyspark and SQL

There are 3 types of datasets in delta live tables -
1.Streaming table
2.Materialized View or Live table
3.View

1.Streaming table:
.Supporting Incremental and streaming ingesion from append only sources.Each record is processed only once.
.Autoloader is used to automatically detect and individually process new files as they arrive.
.Cloud storage and message queues are used in streaming ingestion with autoloader
.High volume data with rapid growth with low latency.
.No re-computation of old data needed
.Suitable for extraction needed

eg: Bronze table in medallion architecture

SQL Syntax:
CREATE STREAMING TABLe customers
AS SELECT * FROM cloud_files("mnt/a/b/","csv")

Pyspark syntax:
@dlt table
def customers():
return (spark.readStream.format("cloudFiles") \
						.option("cloudFiles.format","json") \
						.option("cloudFiles.inferschema",True) \
						.load("mnt/a/b/"))

2.Materialized View or Live table
.Current state of defined query is processed 
.Handling CDC usecases like insert/delete/update

.Result of query is stored as Live table.Manual DML is not supported. All DML's are handled via defined query
.More suitable for transformation and aggregations (like silver and gold)

SQL Syntax:
CREATE live table sales_agg
AS SELECT order_dt,city,products,sum(sales_amount)
FROM LIVE.sales s
JOIN LIVE.customers
WHERE product="laptop"
GROUP BY order_dt,city,products

Pyspark syntax:
@dlt table
def customers():
return (spark.readStream.format("json").load("mnt/a/b/"))

3.View
.Current state of defined query but data will not persisted 
.Views can be referenced only within pipeline where they are defined
.Suitable for intermediate results where data is not needed for end users or downstream applications

SQL Syntax:
CREATE LIVE VIEW V_sales_agg
AS SELECT order_dt,city,products,sum(sales_amount)
FROM LIVE.sales s
JOIN LIVE.customers
WHERE product="laptop"
GROUP BY order_dt,city,products 

Pyspark syntax:
@dlt view
def v_customers():
return (spark.readStream.format("json").load("mnt/a/b/"))

**IMP
Since DLT is declarative approach, 
you can write the code for the silver table first and then the bronze table, and Databricks Delta Live Tables will intelligently understand the order of operations and feed the bronze table first before updating the silver table. This is because Delta Live Tables uses a declarative approach to data pipelining, which means that you specify the desired state of the data, and Delta Live Tables takes care of the rest.

In the declarative approach, you define the logic for transforming and enriching the data in the silver table, and Delta Live Tables will automatically keep the silver table up-to-date as the underlying bronze table changes. This eliminates the need for you to write explicit code to handle the order of operations, and it makes your data pipelines more maintainable and easier to understand.
=====================================================================================================================================================

Unity Catalog:
Unity Catalog is a fine-grained governance solution for data on lakehouse. 

It helps simplify security and governance of your data by providing a central place to administer and audit data access.

Unified catalog: A unified catalog stores all your data, ML models, and analytics artifacts, in addition to metadata for each data object. The unified catalog also blends in data from other catalogs such as an existing Hive metastore.

Unified data access controls: A single and unified permissions model across all data assets and all clouds. This includes attribute-based access control (ABAC) for personally identifiable information (PII).
eg:creating the roles and granting them access

Data isolation: Data isolation can be achieved at multiple levels–environment, storage location, data objects of increasing granularity–without losing the ability to manage access and auditing centrally.

Data auditing: Data access is centrally audited with alerts and monitoring capabilities to promote accountability.
eg: we can access event_logs table for auditing. Create or alter or drop obects statistics

Data quality management: Robust data quality management with built-in quality controls, testing, monitoring, and enforcement to ensure accurate and useful data is available for downstream BI, analytics, and machine learning workloads.

Data lineage: Data lineage to get end-to-end visibility into how data flows in lake house from source to consumption.

Data discovery: Easy data discovery to enable data scientists, data analysts, and data engineers to quickly discover and reference relevant data and accelerate time to value.

Data sharing: Data can be shared across clouds and platforms. Delta Sharing is an open protocol developed by Databricks for secure data sharing with other organizations, or with other teams within your organization, regardless of which computing platforms they use.

We can share delta lake objects like tables,views and notebooks across the regions, platforms, users, and across the cloud providers as well.

eg. steps for data sharing between region1 to region2:
1.go Unity catalog > delta sharing > create recepient
2.provide recepient name and identifier(select current_metastore()) --assuming recepient is from region2
3.create share
4.grant share
5.attach the objects
6.now user in region2 can open it --region2 workspace should have UC enabled to view shared datasets





=====================================================================================================================================================
Steps to create Unity catalog:
1.Create a storage (blob or adls) --this will be used to store metadata
2.Create databricks connector service in Azure
3.Grant the access(eg. blob contributer) to databricks connector for above storage
4.Go to databricks, in admin account settings, create unity catalog and attach the workspace you wish to attach
IMP**You can have one unity catalog per region
	You cannot assign unity catalog created in X region to workspace in Y region
	Each workspace can be attached to only one region
	One Unity catalog metastore can be attached to multiple worskapces
	We dont have to create SQL end point(or warehouse) separately. When you create the compute(cluster) it should have the access to unity catalog.
=====================================================================================================================================================
Azure AD is now Microsoft Entra ID --Here we will create the users, groups, tenants and also invite the other users
IAM--Identify Access Management --Once the user is created in AD, we will grant the access to any service to that perticular user. It can be at subscription level,RG level or individual resource level. Means, attach the specific role(can be inbuilt role or custom role created) to user. So that the user can access this perticular service/resource.

=====================================================================================================================================================
Lakehouse architecture in detail:

The Bronze Layer in Lakehouse Architecture

The Bronze layer is the first and most raw layer in the Lakehouse architecture. It serves as the landing zone for all incoming data, regardless of its format or structure.
The Bronze layer's primary role is to store ingested data in its original state, ensuring its preservation and accessibility for later processing.

Here are some key characteristics of the Bronze layer:

Unvalidated: Data in the Bronze layer is not validated for accuracy or completeness. This means it may contain errors, inconsistencies, or missing values.
Schema-on-read: The Bronze layer typically does not enforce a specific schema on the data. This allows for flexibility and scalability when dealing with diverse data sources.
Append-only: Data is typically appended to the Bronze layer over time, preserving its history and allowing for backtracking and analysis of historical trends.
Low cost: The Bronze layer is often implemented using low-cost storage options like object storage, making it ideal for storing large volumes of data.
-----------------------------------------------------------------------------------------------------------------------------------------------------
Whether you need to maintain one or multiple bronze tables for sources with the same structure depends on several factors:
1. Data Volume and Ingestion Frequency:
2. Data Lineage and Auditability:
3. Data Governance and Security:
4. Future Processing Requirements:
If you anticipate performing different processing or transformations on data from different sources, maintaining separate bronze tables is beneficial. This allows for individual processing pipelines tailored to each source's specific characteristics.
If you plan to apply the same processing logic to all data regardless of source, a single bronze table might be suitable.

5. Cost Considerations:
Creating and managing multiple bronze tables can incur additional storage and compute costs. This needs to be weighed against the benefits of improved data organization and accessibility.
A single bronze table might be more cost-effective for smaller data volumes and simpler processing needs.
-----------------------------------------------------------------------------------------------------------------------------------------------------
The industry standard for historical data maintenance in the Bronze layer depends on your specific use case and data governance requirements. Here are two main approaches:

1. Archiving Historical Data from the Bronze Layer:
Moving historical data out of the Bronze layer and into a dedicated archive storage solution
Maintaining a limited window of data in the Bronze layer for active analysis and processing
Ensuring access controls are in place to restrict access to archived data

Benefits:
Reduces storage costs in the Bronze layer, which typically uses low-cost object storage.
Improves performance and query efficiency by focusing active queries on a smaller subset of data.
Simplifies data management and governance by separating active and historical data.
Ensures compliance with data retention policies and regulations.

Drawbacks:
Requires additional infrastructure and processes for archiving and retrieving data.
May increase complexity for queries that require access to both current and historical data.
Can make it more challenging to track data lineage and provenance across different data stores.
-----------------------------------------------------------------------------------------------------------------------------------------------------
2. Keeping All Historical Data in the Bronze Layer:
Storing all historical data in the Bronze layer alongside the active data.
Partitioning data based on timeframes to optimize query performance.
Implementing metadata management practices to track data lineage and provenance.

Benefits:
Simplifies data management by keeping all data in a single location.
Facilitates access to historical data for analytics and research purposes.
Provides complete data lineage and provenance for all data points.

Drawbacks:
Increases storage costs in the Bronze layer.
May decrease query performance and efficiency, especially for large datasets.
Requires additional effort to manage and maintain a growing data volume
=====================================================================================================================================================
Diving Deeper into the Silver Layer of Lakehouse Architecture:

The Silver layer within a Lakehouse architecture serves as a crucial intermediary between the raw data in the Bronze layer and the curated, analysis-ready data in the Gold layer. It acts as a processing and transformation stage where raw data is enriched, validated, and transformed into a more usable form for downstream analytics and applications.

Here are some key characteristics of the Silver layer:
1. Structured and Enriched Data:
	defined schema and data structure, facilitating efficient querying and analysis.
	joining data from different sources, deduplication, and data quality checks.
	
2. Optimized for Consumption:
	The Silver layer aims to provide data in a format ready for specific use cases and analytic tasks.
	This may involve creating pre-aggregated datasets, materialized views, or calculated metrics for improved query performance.
	
3. Version Control and Lineage Tracking:
	Data in the Silver layer is versioned to track changes and maintain historical context.
	Lineage information is also documented to understand the origin and transformations applied to the data.
	
4. Governance and Security:
	Access control policies are implemented to ensure only authorized users have access to sensitive data in the Silver layer.
	Data governance practices are enforced to ensure the quality, consistency, and reliability of the data.
	
5. Scalability and Performance:
	The Silver layer needs to be scalable to handle large data volumes and support concurrent queries efficiently.
	Techniques like data partitioning and indexing are utilized to optimize query performance.

Transformations beyond cleansing and deduplication:
Data enrichment: Joining data from different sources, calculating derived attributes, and adding context-specific information.
Data aggregation: Pre-aggregating data for specific use cases to improve query performance.
Feature engineering: Creating new features from existing data to enhance machine learning models.
Data normalization: Converting data to a consistent format for easier comparison and analysis.
Data partitioning: Organizing data based on specific criteria for efficient querying and analysis.
Data quality checks: Implementing advanced checks beyond basic validation, like anomaly detection and data completeness analysis.

Maintaining historical data in the Silver layer:
Whether to maintain historical data in the Silver layer depends on various factors:

Frequency of access: If historical data is frequently accessed for analysis, it might be beneficial to keep it in the Silver layer for faster retrieval.
Storage costs: Keeping large volumes of historical data in the Silver layer can incur significant storage costs.
Data governance requirements: Regulations or internal policies might require retaining historical data for specific periods.
Data lifecycle management strategy: Integrating the Silver layer into a broader data lifecycle management strategy helps determine the appropriate retention period for different data subsets.	

Users beyond the Golden layer:
While the primary purpose of the Silver layer is to prepare data for the Golden layer, other users may benefit from accessing it:

Data scientists: Utilize the Silver layer data for exploratory analysis, model development, and feature engineering.
Business analysts: Conduct ad-hoc queries and analyze specific trends or insights.
Data engineers: Monitor data quality, troubleshoot data pipelines, and perform data lineage analysis.
Application developers: Access pre-processed data for building data-driven applications and dashboards.
	
**Move the old data to archive if your business use case is just to extract latest data out of it
=====================================================================================================================================================

Delving into the Golden Layer of Lakehouse Architecture:

The Golden layer sits at the pinnacle of the Lakehouse architecture, representing the curated, analysis-ready data extracted from the raw data in the Bronze layer and the enriched data in the Silver layer. It serves as the single source of truth for business insights and decision-making.

Key characteristics of the Golden layer include:
Highly refined data: The data is cleansed, transformed, and aggregated to fit specific business needs and analytic purposes.
Optimized for query performance: Data structures and indexes are implemented to ensure efficient and fast retrieval for complex queries and data exploration.
Business-oriented: Data is organized and presented in a way that aligns with business users' needs and facilitates informed decision-making.
Governance and security: Strict access control mechanisms and data governance policies are enforced to ensure data integrity and confidentiality.
Versioning and lineage tracking: Every data change is tracked and documented to understand the data's provenance and ensure consistency over time.

Benefits of the Golden layer:
Enhanced data quality and consistency: Provides reliable data for accurate analysis and reporting.
Reduced time to insights: Offers readily available data, eliminating the need for pre-processing before analysis.
Improved business decision-making: Enables data-driven decisions based on reliable and consistent information.
Increased operational efficiency: Streamlines data access and reduces time spent on data preparation tasks.
Enhanced collaboration and communication: Provides a single source of truth for all stakeholders.

Challenges of the Golden layer:
Complexity of development and maintenance: Building and maintaining a robust Golden layer requires careful planning, data modeling, and ongoing maintenance.
Performance considerations: Optimizing data structures and indexing strategies is crucial for ensuring fast query performance.
Data governance overhead: Implementing and enforcing access controls and data quality standards requires effort and resources.
Data staleness risk: Balancing data freshness with the time required for data validation and refinement is essential.

Common Golden layer data models:
Dimensional models: Designed for analyzing business processes and relationships between different entities.
Data marts: Subject-specific data subsets tailored for specific business areas and analytics needs.
Analytical databases: Optimized for fast and efficient querying and data exploration.

Best practices for Golden layer management:
Define clear data governance policies and access control rules.
Establish data quality standards and monitoring procedures.
Implement automated data validation and cleansing processes.
Document data lineage and transformations thoroughly.
Monitor query performance and optimize data structures as needed.
Regularly review and update data models and definitions.
Communicate clearly with stakeholders about data availability and usage guidelines.
=====================================================================================================================================================


Datamesh:
