Round 1:
HackerRank Test:
Difficulty level - Moderate
Thoeritcal questions on Azure and SQL.All are MCQ's
eg:
1.Which Azure service is used for virtual machine deployment and management?
2.What is the primary purpose of Azure Blob Storage?
3.Which Azure service provides a scalable cloud database with multi-region, multi-model database support?
4.What does Azure Resource Manager (ARM) provide in Azure?
5.Which Azure service is designed for big data analytics and allows you to analyze large datasets in real-time?
6.Which Azure service is used for serverless event processing and real-time analytics?
7.DDL's will be given along with scenarios. You need to choose the right SQL query. Need to be extra careful as all queries looks same with negligible differences

Round 2 - Technical(15min)
Difficulty level - Moderate
1.Your career journey
2.Most recent Projets you worked on based on relevant skill set
3.Be prepared with JD and role you applied for
4.Explain any automation or process improvement you have implemented
5.Some high level questions on your relevant skills

Round 3 - Core System Design Points for Data Engineers
Difficulty level - Moderate to High
1. Design a batch pipeline to extract, transform, and load data from source systems into a data lake or warehouse.
2. Build a real-time pipeline using Kafka/Event Hubs and Spark/Flink to process streaming data with low latency.
3. Use a Lakehouse architecture with Bronze, Silver, and Gold layers for scalable and governed analytics.
4. Implement CDC (Change Data Capture) to track and replicate data changes from source to target systems.
5. Handle schema evolution gracefully using columnar formats (Parquet, Avro) with schema registry support.
6. Partition data effectively by date or business key to optimize query performance and scalability.
7. Manage late-arriving data using watermarking and windowed aggregations in streaming engines.
8. Ensure pipeline idempotency to avoid duplicate records during retries or failures.
9. Create metadata-driven pipelines that adapt dynamically using configuration tables.
10. Integrate monitoring and alerting for real-time visibility into pipeline health and performance.
11. Apply data quality checks like null checks, duplicates, and validation rules before writing to final tables.
12. Use data masking and encryption to secure sensitive fields like PII or financial information.
13. Design multi-tenant platforms with compute, storage, and access isolation per client or team.
14. Adopt event-driven triggers for real-time ingestion on file arrival or API notifications.
15. Implement retry and error handling with exponential backoff and dead-letter queues.
16. Enable historical backfill using parameterized jobs that rerun past partitions or time windows.
17. Use staging layers to validate raw data before moving to refined or curated layers.
18. Leverage Delta Lake features like time travel and vacuuming for version control and cleanup.
19. Implement SCD logic using Type 1/2 to track historical changes in dimension tables.
20. Optimize storage and queries using file formats like Delta/Parquet and Z-order indexing.


Round 4 - Technical- Cloud(1hr) - Expect more than 1 interview panel
Difficulty level - High
1.Explain yourself and your career journey
2.Explain your most recent project
3.Talk about azure services involved in any data engineering project. Here you need to pick any realtime scenario and explain end to end.
4.Then expect the questions from point 3.
5.Expect questions on Databricks
eg:
Spark architecture
How many executors will you assign for 10GB file in HDFS?
How many cores are needed for each executor and what amount of memory required for each executor?
what is on-heap memory?what is off-heap memory? which is faster?
Spark API's(RDD,Dataframe,Dataset) and their differences
Handling error records
repartition,Shuffle Partition,coalesce
partitionBy and bucketBy are used while writing only
Cache vs Persist
Broadcast variable, AQE
DELTA LAKE internal mechanism and features
DAG's
Performance optimizations techniques in spark
Delta Live Tables and Unity Catalog
6.Scenarios will be given(panel will share the screen for question) and you need to write Pyspark code for given scenario by shareing your screen
7.Scenarios will be given(panel will share the screen for question) and you need to write SQL Queries for given scenario by shareing your screen
Be well prepared for advanced SQLs' eg: Window functions like Rank, Dense Rank,Row number, LEAD, LAG,CTE's, Aggregations,Joins,Pivoting etc.
8.Scenarios will be given(panel will share the screen for question) and you need to write Python code for given scenario by shareing your screen
Expect questions on data structures like Stings, arrays, lists, tuples, sets, dictionaries and their manipulations
loops and iterations like "for"
Regex

Round 5 – Python (1hr)
Difficulty Level: Moderate to High
Expected Topics:
Core Python ConceptsData types, loops, conditional statements
List comprehension vs traditional loops
*args and **kwargs
Exception handling – try-except-finally blocks
File handling basics
Object-Oriented Programming (OOP)
Class, Object, Inheritance, Polymorphism
Use case scenario-based OOP implementation
Data Manipulation using Python
Working with dictionaries, nested data
JSON parsing
Regular expressions (re module)
Pandas and NumPy
DataFrame operations, filtering, grouping
Handling nulls, merging, joins
Performance tuning in Pandas
Broadcasting and vectorization in NumPy
Automation Scripts
Python scripts for data loading (e.g., to/from Blob, SFTP, REST API)
Sample: Write a script to read a CSV file, remove duplicates, and write to Azure Blob.
Unit Testing and Logging
unittest or pytest usage
Logging best practices using logging module

Scenarios:
You’re given a REST API and asked to write a Python script to extract and store data in a Delta table using PySpark.
Write a function to validate column types in a given schema.

Round 6 – SQL (1hr)
Difficulty Level: High

Expectations:
Advanced SQL Concepts
Window functions (ROW_NUMBER, RANK, LAG/LEAD)
CTEs, recursive CTEs
Complex joins, anti-joins, self-joins
EXISTS vs IN vs JOIN
Subqueries and correlated subqueries
Data Transformation Scenarios
Pivot and unpivot
Aggregations with GROUP BY ROLLUP/CUBE
Conditional aggregations using CASE WHEN
Finding duplicates, gaps in sequences, etc.
Performance Tuning
Indexing (clustered/non-clustered)
Query optimization techniques
Reading execution plans
Transactional Concepts
ACID properties
Isolation levels (Read committed, Snapshot, etc.)
Real-Time Scenarios
You are given sales and product tables and asked to identify top 5 products by category based on average revenue.
Write a query to find the second highest salary in a department without using LIMIT/TOP.
DDL/DML based Scenarios
Spot the mistake in slightly different-looking queries
Write SQL to update a record only if it exists; otherwise, insert it (MERGE logic)


Round 7 - Hiring manager round for 1hr: - Expect more than 1 interview panel
Difficulty level - High
1.Read some blogs or websites to know more about PepsiCo
2.Take any realtime scenario and automate it (in Databricks or Snowflake --your choice)
3.Expect some technical questions too like above for around 20-30min
4.You will be tested in people management area(depends on role you applied)
5.Talk about your strengths and weakness (DO NOT stress more on your weakness)
6.Expect question like, tell me good and bad about PepsiCo(again DO NOT talk any bad about PepsiCo just for sake of answering)
Note some good points about PepsiCo like how it is adapting new technologies into their business, sustainibilty etc.
7.Mske sure you ask any question in the end

Round 8 - HR Round for 30min:
1.What do you know about PepsiCo and the role you are applying for
2. Why Pepsi
3.your career journey
4.Then HR will talk about Benefits






